# 初步测试



在链接之后输入robots.txt查看它是否允许我们爬取，结果如图

![image-20220304140803243](C:\Users\10203\AppData\Roaming\Typora\typora-user-images\image-20220304140803243.png)



可以知道该网站是允许爬取的。

利用requests库以及beautifulsoup初步爬取网页内容，以[留学中介申请常见问题 (compassedu.hk)](https://www.compassedu.hk/wa)为例，结果如图

可以看出该网站没有反爬，其他三个也是一样，都是一种类型，**其中ask只是它的页面内容包括的是申请者常问的一些问题，与其他三个并无不同**

![image-20220304132000251](C:\Users\10203\AppData\Roaming\Typora\typora-user-images\image-20220304132000251.png)

因此，可以得出爬虫思路

先判断网页是否允许爬虫
（1）获取源码
　　1、不允许就加上headers头部信息，模拟用户访问，避免被知道是我们在做爬取工作
　　2、response请求进入网页
　　3、打开网页
　　4、获取源码

（2获取标题或者内容
　　1、进入网页寻找网页中我们寻找的内容部分
　　2、找到标题或者内容那一部分，通过正则表达式匹配该部分
　　3、在整个网页中匹配这个内容
　　4、打印出信息
　　5、调用显示

（3）多页的显示
　　1、看每页的网址的区别，一般是在后面加上+str(page)
　　2、然后分别调用每页的内容

（4）把获取的内容写到文件中保存
　　

**反爬安全措施：**

1.加user -agent,模拟用户访问，不让它知道我们是python访问

2.使用代理Ip，不怕封掉，封掉还有可用的IP

3.降低请求的频率

4.进行模拟登录

